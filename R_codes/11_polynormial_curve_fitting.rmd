---
title: "Polynomial Curve Fitting"
author: "Chenxin Li"
date: "6/21/2020"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction
Last time we talked abou correlation and linear regression.
We also talked about how to fit a curve when the underlying mathematical relationship is given after a linear transformation. 

This time we are going to talk about polynomial curve fitting.
Polynomial curve fitting is a powerful technique.
It allows us to fit curves even without a known underlying mathematical relationship. 

#load packages
```{r}
library(ggplot2) 
library(tidyr)
library(dplyr)
library(readr)
library(readxl)
library(RColorBrewer)
library(viridis)
```


#The principle under polynomial curve fitting

It turns out curves of any shape can be approximated mathematically by the polynomial formula:
y = a + bx + cx^2 + dx^3 + ex^4 ... 
And we just need to find the coefficients (a, b, c, d and e),
as well as the number of terms in the equation (i.e. what's the highest power term of x?).


#How to set up a polynomial regression model? 
We will use the potato yield experiment as an example. 
We are looking at the yield of two potato varieties across 5 nitrogen inputs. 
(Data from UC Davis Plant Sciences course PLS205, Winter 2017) 

```{r}
potato <- read_csv("data/Potato_Yield.csv")
head(potato)
```

We'll use the data for one of the varieties for now.
We'll fit a polynomial regression model for the vareity "Alpine Russet".
```{r}
Alpine <- potato %>% 
  filter(Variety == "Alpine Russet")
```


##visualization
Before we do any analysis, let's visualize the data first

```{r}
Alpine %>% 
  ggplot(aes(x = Nitrogen, y = Yield)) +
  geom_point(alpha = 0.8, 
             position = position_jitter(5, seed = 666)) +
   theme_minimal() +
  theme(axis.line = element_line(size = 1.2)) +
  theme(text = element_text(size = 12, color = "black", face = "bold")) +
  theme(axis.text = element_text(size = 12, color = "black", face = "bold"))
```
This is clearly non-linear; that's why we need to do polynomial curve fitting. 
(Otherwise we would just go with simple linear regression.)

##identify the model
An important aspect of polynomial curve fitting is to determine what is the highest power term of the model.
And it can only be determined using a trial-and-error approach.
One way to do this is to start with the 1st power (x^1), and move up. 

We will compare the following models, and chose the model with the highest adjusted R^2:

Y ~ a + bx;
Y ~ a + bx + cx^2;
Y ~ a + bx + cX^2 + dx^3...

You might ask: when do I stop? 
The math will say: 
if you have n distinct levels of the predictor, the highest power term of the predictor will be n - 1.
However, when you do have x^(n-1) has the highest power term, 
it will produce a curve that connects the mean of every distinct predictor value.
We don't really want a curve that connects every dot. That's not a regression. That's called smoothing. 
So that leaves us with n - 2. 
The rule is: if you have n distinct levels of the predictor, the highest power term of the predictor will be n - 2.

In this case we have 5 different levels of nitrogen, so the highest level we can get 5 - 2 = 3, so x^3.

To specify a polynomial regression model, we need to use the I(x^power) syntax.
```{r}
model_alpine1 <- lm(Yield ~ I(Nitrogen), data = Alpine)
summary(model_alpine1)
```
This is the result of the model only containing x^1.
We can see that the R^2 is 0.14, and adjusted R^2 is 0.12, pretty low. 
This means only 14% of the variance is explained by the data. 



Now let's move on to adding the x^2 power term into the model.
```{r}
model_alpine2 <- lm(Yield ~ I(Nitrogen) + I(Nitrogen^2), data = Alpine)
summary(model_alpine2)
```
Now the R^2 has increased to 0.5, and adjusted R^2 has increased to 0.48. 
This means the model with a x^2 term is better fit than the model with only x^1 term.


Now let's move on to adding the x^3 term into the model
```{r}
model_alpine3 <- lm(Yield ~ I(Nitrogen) + I(Nitrogen^2) + I(Nitrogen^3), data = Alpine)
summary(model_alpine3)
```
Now, something interesting has happen.
Although R^2 increased from 0.5 to 0.51, adjusted R^2 has decreased from 0.48 to 0.47.
This is an indication of over-fitting.
Adjusted R^2 corrects for number of parameters in the model.
As more parameters (terms) are added to the model, R^2 always increases, because there is always variances that are explained by chance. 
Adjusted R^2 takes into account the number of parameters in the model, and penalizes for more parameters. 
More parameters will increase R^2, but more parameters will also decrease adjusted R^2.
In fitting a polynormial regression model, we need to optimize adjusted R^2. 
The higher the adjusted R^2, the more accurately we will capture the true trend of the data.


So it seems like the optimal model will be Y ~ a + bx + cx^2. 
And the equation will be: 
Yield = 340.8 + 1.42 * Nitrogen - 0.0044 * Nitrogen^2, according to the coefficients of the x^2 model. 


Before we proceed, we should check the assumptions first.
Normality?
```{r}
plot(model_alpine2, which = 2)
```

Pretty good.

Equal variance?
```{r}
plot(model_alpine2, which = c(1, 3))
```
Perfect. 


##Visualize the predicted value
Before you go ahead and report your model, you should plot the model to see how well it fits the data first

Again, the steps are:

1) First find the range of the predictor. In this case the predictor is Nitrogen. 
2) Go from the lower end to the higher end of the predictor, one small step at a time (e.g. 0.1 increment at a time).
3) Calculate the predicted values using the equation found by the regression model

```{r}
fitted_alpine <-  data.frame(
  "Nitrogen" = seq(min(Alpine$Nitrogen), max(Alpine$Nitrogen), by = 0.1)
) %>%
  mutate(Yield = 340.8 + 1.42 * Nitrogen - 0.0044*Nitrogen^2  )

head(fitted_alpine)
```

```{r}
Alpine %>% 
  ggplot(aes(x = Nitrogen, y = Yield)) +
  geom_point(alpha = 0.8, 
             position = position_jitter(5, seed = 666)) +
  geom_line(data = fitted_alpine, color = "indianred", size = 1.2) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2)) +
  theme(text = element_text(size = 12, color = "black", face = "bold")) +
  theme(axis.text = element_text(size = 12, color = "black", face = "bold"))
```

You can see that the trend is accurately captured by the curve we fitted.
You may noiced that there is a lot of spread at each nitrogen level. That's cause of the low R^2.
This is very common in actual experiments. It is what it is. So don't worry about it. 

##predict intermediate levels and maximum 
Lastly, we can predict any intermediate values we want. 
Say we are interested in which nitrogen input gives the maximum yield.
We can take the first derivative of the equantion, and find where f'(x) = 0, and solve for the x value at that point.

The equation is:
Yield = 340.8 + 1.42 * Nitrogen - 0.0044 * Nitrogen^2  

So dy/dx = 1.42 - 0.0088 * x. Solving for dy/dx = 0 
```{r}
#x = 
1.42/0.0088
```

Looks like it will be 161.4. 
Looking back at the curve, that seems to make sense.

So the predicted maximum yield will be:
```{r}
340.8 + 1.42 * 161.4 - 0.0044*161.4^2  
```
...will be 455. 


#Be careful with polynomial curves
After you fit a polynomial curve, you DO NOT want to extrapolate beyond your predictor's range. 
In this example, the lowest nitrogen input is 0, and the highest is 270. 
You DO NOT want to run your model with predictor below 0 or above 270. 
If you do that, you will get results that make no sense.  

Let me show you what will happen if you do that:

```{r}
fitted_alpine_extended <-  data.frame(
  "Nitrogen" = seq(0, 600, by = 0.1)
) %>%
  mutate(Yield = 340.8 + 1.42 * Nitrogen - 0.0044*Nitrogen^2  )
```

I extended the range of predictor from (0, 270) to (0, 600). Let's see what will happen. 
```{r}
Alpine %>% 
  ggplot(aes(x = Nitrogen, y = Yield)) +
  geom_point(alpha = 0.8, 
             position = position_jitter(5, seed = 666)) +
  geom_line(data = fitted_alpine_extended, color = "indianred", size = 1.2) +
  theme_minimal() +
  theme(axis.line = element_line(size = 1.2)) +
  theme(text = element_text(size = 12, color = "black", face = "bold")) +
  theme(axis.text = element_text(size = 12, color = "black", face = "bold"))
```
You will see that the yield rapidly drops as nitrogen increases.
This is against common sense, since nitrogen should promote yield, 
not to mention negative yields are biologically impossible. 


#Exercise
Now that you have learned how to do polynomial curve fitting,
it's time for you to practice.

This time we'll use the data for the variety "Russet Burbank" in the potato yield data.
```{r}
Burbank <- potato %>% 
  filter(Variety == "Russet Burbank")
```

##Fit a polynormial model

Make the following models:
Y ~ a + bx, 
Y ~ a + bx + cx^2, and
Y ~ a + bx + Cx^2 + dx^3 

Identify the optimal model among them. 

What is the equation for the polynomial? 

What is the adjusted R^2? 

##visualize the model
Draw out the model along with all the dots

Make your plot here:
```{r}

```








##make prediction
What is the predicted yield when nitrogen = 220? 
```{r}

```
















